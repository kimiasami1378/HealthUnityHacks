"""
Problem Description
Mental health challenges and work-related stress are pervasive issues that impact individuals across various professions and demographics. Research grounded in Hopelessness Theory and Effortâ€“Reward Imbalance demonstrates a clear relationship between general work stress, suicide cognitions, hopelessness, and job satisfaction. Stress is a significant predictor of suicide cognitions, with empirical evidence linking high work stress to increased hopelessness and decreased job satisfaction, both of which mediate its effects on mental health outcomes.

Beyond its direct impact on mental well-being, stress exacerbates physical ailments, reduces productivity, and diminishes overall quality of life. To address these critical challenges, an AI-powered, data-driven system can play a pivotal role in identifying, analyzing, and mitigating stress, leveraging insights derived from behavioral patterns, temporal trends, and emotional states.

Framework for Problem Solving
This solution integrates evidence-based insights from mental health research, emphasizing the therapeutic and diagnostic potential of journaling. Studies reveal that journaling has profound benefits, including:

Reducing anxiety and obsessive thinking.
Encouraging self-awareness and emotional regulation.
Enhancing psychological well-being and mood.
Decreasing stress-related doctor visits and absenteeism.
Improving cognitive and emotional outcomes like resilience and positivity.
Additionally, research highlights the mediating roles of hopelessness and job satisfaction in the stress-cognition relationship. By capturing these insights through structured journaling, this system provides real-time, actionable feedback to improve emotional well-being.

Why This Solution Matters
This AI-powered journaling system tackles stress and mental health challenges by:

Reducing Hopelessness: Providing immediate insights into emotional states and identifying triggers helps reduce feelings of hopelessness by empowering users with actionable strategies.
Boosting Job Satisfaction: Offering personalized recommendations based on temporal patterns and stressors enhances self-management, leading to improved job satisfaction.
Encouraging Emotional Catharsis: By facilitating expressive writing, users experience an emotional release, supporting emotional processing and reducing psychological distress.
Target Use Cases
High-Stress Professionals: Offering tools to track and manage work-related stress while providing recommendations to maintain emotional balance and productivity.
Individuals with Mental Health Concerns: Enabling early identification of patterns in stress, hopelessness, and negative emotions, providing actionable strategies to mitigate their impact.
Clinicians and Care Teams: Equipping healthcare providers with detailed insights and trends for personalized patient care.
"""

#1. Dataset Loading and Preprocessing

import pandas as pd
import logging
from datetime import datetime
from openCHA import openCHA
from openCHA.tasks.task import BaseTask
from openCHA.datapipes import DataPipe
from transformers import pipeline
import json
from datetime import datetime
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import seaborn as sns

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Define dataset paths
dataset_paths = {
    "labels": "/mnt/data/500_Reddit_users_posts_labels.csv",
    "attempt": "/mnt/data/suicidal_attempt.csv",
    "behavior": "/mnt/data/suicidal_behavior.csv",
    "ideation": "/mnt/data/suicidal_ideation.csv",
    "indicator": "/mnt/data/suicidal_indicator.csv",
    "batch1": "/mnt/data/Redditors_and_posts_batch_1.xlsx",
    "batch2": "/mnt/data/Redditors_and_posts_batch_2.xlsx",
    "batch3": "/mnt/data/Redditors_and_posts_batch_3.xlsx",
    "batch4": "/mnt/data/Redditors_and_posts_batch_4.xlsx",
}

# Load datasets with error handling
data_frames = []
for name, path in dataset_paths.items():
    try:
        if path.endswith(".csv"):
            df = pd.read_csv(path)
        elif path.endswith(".xlsx"):
            df = pd.read_excel(path)
        else:
            raise ValueError(f"Unsupported file type for {path}")
        
        # Add source column and handle timestamps if available
        df['source'] = name
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        
        data_frames.append(df)
        logging.info(f"Successfully loaded {name}")
    except Exception as e:
        logging.error(f"Error loading {name}: {e}")

# Combine all datasets
combined_data = pd.concat(data_frames, ignore_index=True)

# Standardize column names
combined_data.columns = [col.strip().lower().replace(" ", "_") for col in combined_data.columns]

# Save cleaned dataset
combined_data.to_csv("cleaned_combined_data.csv", index=False)
logging.info("Combined dataset saved as cleaned_combined_data.csv.")

#2. Task Definitions
#2.1. Daily Insights Task

from openCHA.tasks.task import BaseTask
import json

class DailyInsightsTask(BaseTask):
    def __init__(self, nlp_pipeline, datapipe=None):
        super().__init__(
            name="daily_insights",
            chat_name="DailyInsights",
            description="Provide daily sentiment insights and important keywords.",
            inputs=["JSON object of daily journal entries."],
            outputs=["Daily sentiment and keyword analysis."],
            datapipe=datapipe,
            output_type=False
        )
        self.nlp_pipeline = nlp_pipeline

    def _execute(self, inputs):
        daily_data = json.loads(inputs[0])
        insights = []

        for entry in daily_data:
            sentiment = self.nlp_pipeline.analyze_sentiment(entry['Text'])
            keywords = self.nlp_pipeline.extract_keywords(entry['Text'])
            insights.append({
                "Date": entry['Date'],
                "Sentiment": sentiment['label'],
                "Score": sentiment['score'],
                "Keywords": keywords
            })

        return json.dumps(insights)

    def explain(self):
        return "Uses NLP pipeline to generate daily sentiment and keyword insights."

#2.2. Weekly/Monthly Trend Analysis Task

from collections import Counter

class WeeklyTrendAnalysisTask(BaseTask):
    def __init__(self, datapipe=None):
        super().__init__(
            name="weekly_trend_analysis",
            chat_name="WeeklyTrends",
            description="Analyze weekly sentiment trends and recurring keywords.",
            inputs=["JSON object of weekly journal entries."],
            outputs=["Weekly sentiment and keyword trends."],
            datapipe=datapipe,
            output_type=False
        )

    def _execute(self, inputs):
        weekly_data = json.loads(inputs[0])
        sentiment_counts = Counter()
        keyword_counts = Counter()

        for entry in weekly_data:
            sentiment_counts.update([entry['Sentiment']])
            keyword_counts.update(entry['Keywords'])

        trends = {
            "SentimentTrends": dict(sentiment_counts),
            "TopKeywords": keyword_counts.most_common(10)
        }
        return json.dumps(trends)

    def explain(self):
        return "Analyzes weekly sentiment and keyword trends."

#2.3. Temporal Pattern Analysis Task

class TemporalPatternAnalysisTask(BaseTask):
    def __init__(self, datapipe=None):
        super().__init__(
            name="temporal_pattern_analysis",
            chat_name="TemporalPatternAnalysis",
            description="Detects time-of-day and day-of-week patterns for stress indicators.",
            inputs=["A JSON object with daily entries including timestamp."],
            outputs=["Temporal pattern insights."],
            datapipe=datapipe,
            output_type=False
        )

    def _execute(self, inputs):
        daily_data = json.loads(inputs[0])
        time_of_day_counts = Counter()
        day_of_week_counts = Counter()

        for entry in daily_data:
            timestamp = datetime.fromisoformat(entry['Timestamp'])
            time_of_day = "morning" if 6 <= timestamp.hour < 12 else \
                          "afternoon" if 12 <= timestamp.hour < 18 else \
                          "evening" if 18 <= timestamp.hour < 22 else "night"
            day_of_week = timestamp.strftime('%A')

            time_of_day_counts[time_of_day] += 1
            day_of_week_counts[day_of_week] += 1

        return json.dumps({
            "TimeOfDayPattern": dict(time_of_day_counts),
            "DayOfWeekPattern": dict(day_of_week_counts)
        })

    def explain(self):
        return "Analyzes temporal patterns in stress indicators."

#2.4. Personalized Recommendations Task

class PersonalizedRecommendationsTask(BaseTask):
    def __init__(self, datapipe=None):
        super().__init__(
            name="personalized_recommendations",
            chat_name="Recommendations",
            description="Generate recommendations based on temporal and emotional trends.",
            inputs=["Daily insights, weekly trends, and temporal patterns."],
            outputs=["Recommendations with specific justifications."],
            datapipe=datapipe,
            output_type=False
        )

    def _execute(self, inputs):
        insights, trends, temporal_patterns = json.loads(inputs[0]), json.loads(inputs[1]), json.loads(inputs[2])
        recommendations = []

        # Analyze sentiments and emotions for recommendation
        if any(entry['Sentiment'] == "Negative" and entry['Emotion'] in ["sadness", "anger"] for entry in insights):
            recommendations.append({
                "Recommendation": "Try mindfulness for 10 minutes daily.",
                "Justification": "Recurring sadness detected in emotional trends."
            })

        if "Monday" in temporal_patterns["DayOfWeekPattern"]:
            recommendations.append({
                "Recommendation": "Start Mondays with a gratitude exercise to counter the 'Monday blues'.",
                "Justification": "Weekly pattern shows heightened stress on Mondays."
            })

        return json.dumps(recommendations)

    def explain(self):
        return "Generates actionable recommendations from emotional and temporal trends."

#3. Orchestrator Initialization and Execution

from openCHA.orchestrator import Orchestrator

orchestrator = Orchestrator.initialize(
    planner_llm="gpt-4",
    planner_name="tree_of_thought",
    datapipe_name="memory",
    response_generator_name="base_generator",
    available_tasks=[
        DailyInsightsTask,
        WeeklyTrendAnalysisTask,
        TemporalPatternAnalysisTask,
        PersonalizedRecommendationsTask,
    ],
)

daily_data = json.dumps([
    {"Date": "2024-11-14", "Timestamp": "2024-11-14T09:30:00", "Text": "Feeling anxious about work deadlines."},
    {"Date": "2024-11-15", "Timestamp": "2024-11-15T14:45:00", "Text": "Had a productive day but feeling drained."}
])

weekly_data = json.dumps([
    {"Date": "2024-11-08", "Sentiment": "Negative", "Keywords": ["overwhelmed", "work"]},
    {"Date": "2024-11-09", "Sentiment": "Neutral", "Keywords": ["productive", "project"]}
])

# Execute tasks
daily_results = orchestrator.execute_task("daily_insights", [daily_data])
weekly_trends = orchestrator.execute_task("weekly_trend_analysis", [weekly_data])
recommendations = orchestrator.execute_task("personalized_recommendations", [daily_results, weekly_trends])

print("Daily Insights:", daily_results)
print("Weekly Trends:", weekly_trends)
print("Recommendations:", recommendations)

"""
Visualizations
Ensuring visualizations are meaningful and tied to insights:

"""

# Sentiment Trends Heatmap
pivot_sentiment = combined_data.pivot_table(
    index=pd.to_datetime(combined_data['timestamp']).dt.date,
    columns='sentiment',
    values='user_id',
    aggfunc='count',
    fill_value=0
)

# Normalize and plot heatmap
pivot_sentiment_normalized = pivot_sentiment.div(pivot_sentiment.sum(axis=1), axis=0)
plt.figure(figsize=(12, 6))
sns.heatmap(pivot_sentiment_normalized, cmap='coolwarm', annot=True, fmt=".2f")
plt.title("Sentiment Trends Over Time")
plt.tight_layout()
plt.show()

"""Predictive Analytics (Future Stress Levels): 
Integrating ML models like Linear Regression  to predict stress levels: """

from sklearn.linear_model import LinearRegression
import numpy as np

# Prepare data for prediction
combined_data['timestamp_numeric'] = combined_data['timestamp'].apply(lambda x: x.timestamp())
X = combined_data[['timestamp_numeric']].values
y = combined_data['sentiment_score'].values

# Train a simple regression model
model = LinearRegression()
model.fit(X, y)

# Predict future stress levels
future_timestamps = np.array([(datetime.now().timestamp() + i * 86400) for i in range(7)]).reshape(-1, 1)
predicted_scores = model.predict(future_timestamps)
print("Predicted Stress Levels:", predicted_scores)
